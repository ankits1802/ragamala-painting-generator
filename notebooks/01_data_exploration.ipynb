{
 "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "# Ragamala Paintings - Exploratory Data Analysis and Visualization\n",
        "\n",
        "This notebook provides comprehensive exploratory data analysis (EDA) for the Ragamala painting dataset.\n",
        "We'll analyze the distribution of ragas, styles, periods, and visual characteristics to understand\n",
        "the dataset structure and inform our SDXL fine-tuning approach.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading and Initial Exploration](#data-loading)\n",
        "2. [Dataset Overview and Statistics](#dataset-overview)\n",
        "3. [Raga Distribution Analysis](#raga-analysis)\n",
        "4. [Style and Period Analysis](#style-analysis)\n",
        "5. [Image Characteristics Analysis](#image-analysis)\n",
        "6. [Cultural Context Analysis](#cultural-analysis)\n",
        "7. [Text Prompt Analysis](#prompt-analysis)\n",
        "8. [Data Quality Assessment](#quality-assessment)\n",
        "9. [Insights and Recommendations](#insights)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Image processing libraries\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from skimage import color, feature, measure\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Text processing\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Interactive plotting\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append(str(Path.cwd().parent))\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## 1. Data Loading and Initial Exploration {#data-loading}"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Define data paths\n",
        "DATA_DIR = Path('../data')\n",
        "RAW_DATA_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
        "METADATA_DIR = DATA_DIR / 'metadata'\n",
        "\n",
        "# Load metadata\n",
        "metadata_file = METADATA_DIR / 'metadata.jsonl'\n",
        "\n",
        "def load_metadata(file_path):\n",
        "    \"\"\"Load metadata from JSONL file.\"\"\"\n",
        "    data = []\n",
        "    if file_path.exists():\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    data.append(json.loads(line.strip()))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error parsing line: {e}\")\n",
        "                    continue\n",
        "    return data\n",
        "\n",
        "# Load the metadata\n",
        "metadata_list = load_metadata(metadata_file)\n",
        "print(f\"Loaded {len(metadata_list)} records from metadata file\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "if metadata_list:\n",
        "    df = pd.DataFrame(metadata_list)\n",
        "    print(f\"DataFrame shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "else:\n",
        "    print(\"No metadata found. Creating sample data for demonstration...\")\n",
        "    # Create sample data for demonstration\n",
        "    np.random.seed(42)\n",
        "    n_samples = 500\n",
        "    \n",
        "    ragas = ['bhairav', 'yaman', 'malkauns', 'darbari', 'bageshri', 'todi', 'puriya', 'marwa']\n",
        "    styles = ['rajput', 'pahari', 'deccan', 'mughal']\n",
        "    periods = ['16th_century', '17th_century', '18th_century', '19th_century']\n",
        "    sources = ['Metropolitan Museum', 'British Museum', 'V&A Museum', 'LACMA', 'Private Collection']\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'filename': [f'ragamala_{i:04d}.jpg' for i in range(n_samples)],\n",
        "        'raga': np.random.choice(ragas, n_samples),\n",
        "        'style': np.random.choice(styles, n_samples),\n",
        "        'period': np.random.choice(periods, n_samples),\n",
        "        'source': np.random.choice(sources, n_samples),\n",
        "        'width': np.random.randint(800, 2000, n_samples),\n",
        "        'height': np.random.randint(800, 2000, n_samples),\n",
        "        'file_size_mb': np.random.uniform(0.5, 5.0, n_samples),\n",
        "        'quality_score': np.random.uniform(0.6, 1.0, n_samples),\n",
        "        'has_text': np.random.choice([True, False], n_samples, p=[0.3, 0.7]),\n",
        "        'dominant_colors': [np.random.choice(['red', 'blue', 'gold', 'green', 'white'], 3).tolist() for _ in range(n_samples)]\n",
        "    })\n",
        "    \n",
        "    # Add derived columns\n",
        "    df['aspect_ratio'] = df['width'] / df['height']\n",
        "    df['total_pixels'] = df['width'] * df['height']\n",
        "    df['region'] = df['style'].map({\n",
        "        'rajput': 'Rajasthan',\n",
        "        'pahari': 'Himachal Pradesh',\n",
        "        'deccan': 'Deccan Plateau',\n",
        "        'mughal': 'Northern India'\n",
        "    })\n",
        "\n",
        "print(f\"Final DataFrame shape: {df.shape}\")\n",
        "df.head()"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Display basic information about the dataset\n",
        "print(\"=== DATASET INFORMATION ===\")\n",
        "print(f\"Total number of images: {len(df)}\")\n",
        "print(f\"Number of columns: {len(df.columns)}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "print(\"\\n=== COLUMN DATA TYPES ===\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n=== MISSING VALUES ===\")\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_data,\n",
        "    'Missing Percentage': missing_percent\n",
        "})\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "print(\"\\n=== BASIC STATISTICS ===\")\n",
        "print(df.describe())"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## 2. Dataset Overview and Statistics {#dataset-overview}"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Create comprehensive overview visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Ragamala Dataset Overview', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Source distribution\n",
        "source_counts = df['source'].value_counts()\n",
        "axes[0, 0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 0].set_title('Distribution by Source')\n",
        "\n",
        "# 2. Period distribution\n",
        "period_counts = df['period'].value_counts()\n",
        "axes[0, 1].bar(period_counts.index, period_counts.values, color='skyblue')\n",
        "axes[0, 1].set_title('Distribution by Period')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Quality score distribution\n",
        "axes[0, 2].hist(df['quality_score'], bins=20, alpha=0.7, color='green', edgecolor='black')\n",
        "axes[0, 2].set_title('Quality Score Distribution')\n",
        "axes[0, 2].set_xlabel('Quality Score')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "\n",
        "# 4. File size distribution\n",
        "axes[1, 0].hist(df['file_size_mb'], bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
        "axes[1, 0].set_title('File Size Distribution')\n",
        "axes[1, 0].set_xlabel('File Size (MB)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "\n",
        "# 5. Aspect ratio distribution\n",
        "axes[1, 1].hist(df['aspect_ratio'], bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[1, 1].set_title('Aspect Ratio Distribution')\n",
        "axes[1, 1].set_xlabel('Aspect Ratio (Width/Height)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "\n",
        "# 6. Text presence\n",
        "text_counts = df['has_text'].value_counts()\n",
        "axes[1, 2].bar(['No Text', 'Has Text'], [text_counts[False], text_counts[True]], \n",
        "               color=['red', 'green'], alpha=0.7)\n",
        "axes[1, 2].set_title('Text Presence in Images')\n",
        "axes[1, 2].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"=== DATASET SUMMARY STATISTICS ===\")\n",
        "print(f\"Average image dimensions: {df['width'].mean():.0f} x {df['height'].mean():.0f} pixels\")\n",
        "print(f\"Average file size: {df['file_size_mb'].mean():.2f} MB\")\n",
        "print(f\"Average quality score: {df['quality_score'].mean():.3f}\")\n",
        "print(f\"Average aspect ratio: {df['aspect_ratio'].mean():.3f}\")\n",
        "print(f\"Images with text: {df['has_text'].sum()} ({df['has_text'].mean()*100:.1f}%)\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## 3. Raga Distribution Analysis {#raga-analysis}"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Analyze raga distribution and characteristics\n",
        "raga_analysis = df.groupby('raga').agg({\n",
        "    'filename': 'count',\n",
        "    'quality_score': ['mean', 'std'],\n",
        "    'file_size_mb': 'mean',\n",
        "    'width': 'mean',\n",
        "    'height': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "raga_analysis.columns = ['Count', 'Avg_Quality', 'Quality_Std', 'Avg_FileSize', 'Avg_Width', 'Avg_Height']\n",
        "raga_analysis = raga_analysis.sort_values('Count', ascending=False)\n",
        "\n",
        "print(\"=== RAGA DISTRIBUTION ANALYSIS ===\")\n",
        "print(raga_analysis)\n",
        "\n",
        "# Create comprehensive raga visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Raga Distribution Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Raga frequency\n",
        "raga_counts = df['raga'].value_counts()\n",
        "axes[0, 0].barh(raga_counts.index, raga_counts.values, color='lightcoral')\n",
        "axes[0, 0].set_title('Number of Images per Raga')\n",
        "axes[0, 0].set_xlabel('Count')\n",
        "\n",
        "# 2. Quality score by raga\n",
        "df.boxplot(column='quality_score', by='raga', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Quality Score Distribution by Raga')\n",
        "axes[0, 1].set_xlabel('Raga')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. File size by raga\n",
        "df.boxplot(column='file_size_mb', by='raga', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('File Size Distribution by Raga')\n",
        "axes[1, 0].set_xlabel('Raga')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 4. Raga-Style cross-tabulation heatmap\n",
        "raga_style_crosstab = pd.crosstab(df['raga'], df['style'])\n",
        "sns.heatmap(raga_style_crosstab, annot=True, fmt='d', cmap='YlOrRd', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Raga-Style Distribution Heatmap')\n",
        "axes[1, 1].set_xlabel('Style')\n",
        "axes[1, 1].set_ylabel('Raga')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical analysis of raga distribution\n",
        "print(\"\\n=== RAGA STATISTICAL ANALYSIS ===\")\n",
        "chi2, p_value, dof, expected = chi2_contingency(raga_style_crosstab)\n",
        "print(f\"Chi-square test for Raga-Style independence:\")\n",
        "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "print(f\"Degrees of freedom: {dof}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Ragas and styles are NOT independent (p < 0.05)\")\n",
        "else:\n",
        "    print(\"Ragas and styles appear to be independent (p >= 0.05)\")"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Create interactive raga analysis with Plotly\n",
        "# Raga distribution pie chart\n",
        "fig_raga_pie = px.pie(values=raga_counts.values, names=raga_counts.index, \n",
        "                      title='Interactive Raga Distribution')\n",
        "fig_raga_pie.update_traces(textposition='inside', textinfo='percent+label')\n",
        "fig_raga_pie.show()\n",
        "\n",
        "# Quality score vs file size by raga\n",
        "fig_scatter = px.scatter(df, x='quality_score', y='file_size_mb', color='raga',\n",
        "                        size='total_pixels', hover_data=['width', 'height'],\n",
        "                        title='Quality Score vs File Size by Raga')\n",
        "fig_scatter.show()\n",
        "\n",
        "# Raga characteristics radar chart\n",
        "raga_stats = df.groupby('raga').agg({\n",
        "    'quality_score': 'mean',\n",
        "    'file_size_mb': 'mean',\n",
        "    'aspect_ratio': 'mean',\n",
        "    'total_pixels': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Normalize values for radar chart\n",
        "for col in ['quality_score', 'file_size_mb', 'aspect_ratio', 'total_pixels']:\n",
        "    raga_stats[f'{col}_norm'] = (raga_stats[col] - raga_stats[col].min()) / (raga_stats[col].max() - raga_stats[col].min())\n",
        "\n",
        "print(\"\\n=== RAGA CHARACTERISTICS (Normalized) ===\")\n",
        "print(raga_stats[['raga', 'quality_score_norm', 'file_size_mb_norm', 'aspect_ratio_norm', 'total_pixels_norm']])"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## 4. Style and Period Analysis {#style-analysis}"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Comprehensive style analysis\n",
        "style_analysis = df.groupby('style').agg({\n",
        "    'filename': 'count',\n",
        "    'quality_score': ['mean', 'std'],\n",
        "    'file_size_mb': 'mean',\n",
        "    'aspect_ratio': 'mean',\n",
        "    'has_text': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "style_analysis.columns = ['Count', 'Avg_Quality', 'Quality_Std', 'Avg_FileSize', 'Avg_AspectRatio', 'Text_Percentage']\n",
        "style_analysis = style_analysis.sort_values('Count', ascending=False)\n",
        "\n",
        "print(\"=== STYLE DISTRIBUTION ANALYSIS ===\")\n",
        "print(style_analysis)\n",
        "\n",
        "# Create style visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Style and Period Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Style distribution\n",
        "style_counts = df['style'].value_counts()\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "axes[0, 0].pie(style_counts.values, labels=style_counts.index, autopct='%1.1f%%', \n",
        "               colors=colors, startangle=90)\n",
        "axes[0, 0].set_title('Distribution by Style')\n",
        "\n",
        "# 2. Period distribution\n",
        "period_counts = df['period'].value_counts()\n",
        "axes[0, 1].bar(period_counts.index, period_counts.values, color='lightblue')\n",
        "axes[0, 1].set_title('Distribution by Period')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Style-Period relationship\n",
        "style_period_crosstab = pd.crosstab(df['style'], df['period'])\n",
        "sns.heatmap(style_period_crosstab, annot=True, fmt='d', cmap='Blues', ax=axes[0, 2])\n",
        "axes[0, 2].set_title('Style-Period Distribution')\n",
        "axes[0, 2].set_xlabel('Period')\n",
        "axes[0, 2].set_ylabel('Style')\n",
        "\n",
        "# 4. Quality score by style\n",
        "df.boxplot(column='quality_score', by='style', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Quality Score by Style')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 5. Aspect ratio by style\n",
        "df.boxplot(column='aspect_ratio', by='style', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Aspect Ratio by Style')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 6. Text presence by style\n",
        "text_by_style = df.groupby('style')['has_text'].mean()\n",
        "axes[1, 2].bar(text_by_style.index, text_by_style.values, color='orange', alpha=0.7)\n",
        "axes[1, 2].set_title('Text Presence by Style')\n",
        "axes[1, 2].set_ylabel('Proportion with Text')\n",
        "axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Regional analysis\n",
        "print(\"\\n=== REGIONAL ANALYSIS ===\")\n",
        "regional_analysis = df.groupby('region').agg({\n",
        "    'filename': 'count',\n",
        "    'quality_score': 'mean',\n",
        "    'raga': lambda x: x.nunique()\n",
        "}).round(3)\n",
        "regional_analysis.columns = ['Image_Count', 'Avg_Quality', 'Unique_Ragas']\n",
        "print(regional_analysis)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## 5. Image Characteristics Analysis {#image-analysis}"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Analyze image technical characteristics\n",
        "print(\"=== IMAGE CHARACTERISTICS ANALYSIS ===\")\n",
        "\n",
        "# Resolution analysis\n",
        "df['resolution_category'] = pd.cut(df['total_pixels'], \n",
        "                                  bins=[0, 1000000, 2000000, 4000000, float('inf')],\n",
        "                                  labels=['Low (<1MP)', 'Medium (1-2MP)', 'High (2-4MP)', 'Very High (>4MP)'])\n",
        "\n",
        "resolution_stats = df.groupby('resolution_category').agg({\n",
        "    'filename': 'count',\n",
        "    'quality_score': 'mean',\n",
        "    'file_size_mb': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "print(\"Resolution Category Analysis:\")\n",
        "print(resolution_stats)\n",
        "\n",
        "# Create image characteristics visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Image Technical Characteristics', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Width vs Height scatter\n",
        "scatter = axes[0, 0].scatter(df['width'], df['height'], c=df['quality_score'], \n",
        "                            cmap='viridis', alpha=0.6)\n",
        "axes[0, 0].set_xlabel('Width (pixels)')\n",
        "axes[0, 0].set_ylabel('Height (pixels)')\n",
        "axes[0, 0].set_title('Image Dimensions (colored by quality)')\n",
        "plt.colorbar(scatter, ax=axes[0, 0], label='Quality Score')\n",
        "\n",
        "# 2. File size vs total pixels\n",
        "axes[0, 1].scatter(df['total_pixels']/1000000, df['file_size_mb'], alpha=0.6, color='red')\n",
        "axes[0, 1].set_xlabel('Total Pixels (Megapixels)')\n",
        "axes[0, 1].set_ylabel('File Size (MB)')\n",
        "axes[0, 1].set_title('File Size vs Resolution')\n",
        "\n",
        "# 3. Resolution category distribution\n",
        "resolution_counts = df['resolution_category'].value_counts()\n",
        "axes[0, 2].bar(range(len(resolution_counts)), resolution_counts.values, \n",
        "               color='lightgreen', alpha=0.7)\n",
        "axes[0, 2].set_xticks(range(len(resolution_counts)))\n",
        "axes[0, 2].set_xticklabels(resolution_counts.index, rotation=45)\n",
        "axes[0, 2].set_title('Resolution Category Distribution')\n",
        "axes[0, 2].set_ylabel('Count')\n",
        "\n",
        "# 4. Quality score distribution by resolution\n",
        "df.boxplot(column='quality_score', by='resolution_category', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Quality Score by Resolution')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 5. Aspect ratio distribution\n",
        "axes[1, 1].hist(df['aspect_ratio'], bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[1, 1].axvline(df['aspect_ratio'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {df[\"aspect_ratio\"].mean():.3f}')\n",
        "axes[1, 1].axvline(df['aspect_ratio'].median(), color='orange', linestyle='--', \n",
        "                   label=f'Median: {df[\"aspect_ratio\"].median():.3f}')\n",
        "axes[1, 1].set_xlabel('Aspect Ratio')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Aspect Ratio Distribution')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "# 6. File size distribution by style\n",
        "for style in df['style'].unique():\n",
        "    style_data = df[df['style'] == style]['file_size_mb']\n",
        "    axes[1, 2].hist(style_data, alpha=0.6, label=style, bins=15)\n",
        "axes[1, 2].set_xlabel('File Size (MB)')\n",
        "axes[1, 2].set_ylabel('Frequency')\n",
        "axes[1, 2].set_title('File Size Distribution by Style')\n",
        "axes[1, 2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis\n",
        "print(\"\\n=== CORRELATION ANALYSIS ===\")\n",
        "numeric_cols = ['width', 'height', 'file_size_mb', 'quality_score', 'aspect_ratio', 'total_pixels']\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "            square=True, fmt='.3f')\n",
        "plt.title('Correlation Matrix of Image Characteristics')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Strong correlations (|r| > 0.7):\")\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_val = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.7:\n",
        "            print(f\"{correlation_matrix.columns[i]} - {correlation_matrix.columns[j]}: {corr_val:.3f}\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## 6. Cultural Context Analysis {#cultural-analysis}"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Analyze cultural context and relationships\n",
        "print(\"=== CULTURAL CONTEXT ANALYSIS ===\")\n",
        "\n",
        "# Define cultural mappings\n",
        "raga_time_mapping = {\n",
        "    'bhairav': 'dawn',\n",
        "    'yaman': 'evening',\n",
        "    'malkauns': 'midnight',\n",
        "    'darbari': 'night',\n",
        "    'bageshri': 'night',\n",
        "    'todi': 'morning',\n",
        "    'puriya': 'evening',\n",
        "    'marwa': 'sunset'\n",
        "}\n",
        "\n",
        "raga_mood_mapping = {\n",
        "    'bhairav': 'devotional',\n",
        "    'yaman': 'romantic',\n",
        "    'malkauns': 'meditative',\n",
        "    'darbari': 'regal',\n",
        "    'bageshri': 'romantic',\n",
        "    'todi': 'enchanting',\n",
        "    'puriya': 'mysterious',\n",
        "    'marwa': 'intense'\n",
        "}\n",
        "\n",
        "# Add cultural context to dataframe\n",
        "df['time_of_day'] = df['raga'].map(raga_time_mapping)\n",
        "df['mood'] = df['raga'].map(raga_mood_mapping)\n",
        "\n",
        "# Cultural analysis\n",
        "cultural_analysis = df.groupby(['style', 'period']).agg({\n",
        "    'filename': 'count',\n",
        "    'raga': lambda x: x.nunique(),\n",
        "    'quality_score': 'mean'\n",
        "}).round(3)\n",
        "cultural_analysis.columns = ['Image_Count', 'Unique_Ragas', 'Avg_Quality']\n",
        "\n",
        "print(\"Style-Period Cultural Analysis:\")\n",
        "print(cultural_analysis)\n",
        "\n",
        "# Create cultural context visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Cultural Context Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Time of day distribution\n",
        "time_counts = df['time_of_day'].value_counts()\n",
        "axes[0, 0].pie(time_counts.values, labels=time_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 0].set_title('Distribution by Time of Day')\n",
        "\n",
        "# 2. Mood distribution\n",
        "mood_counts = df['mood'].value_counts()\n",
        "axes[0, 1].bar(mood_counts.index, mood_counts.values, color='lightcoral', alpha=0.7)\n",
        "axes[0, 1].set_title('Distribution by Mood')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Style-Mood relationship\n",
        "style_mood_crosstab = pd.crosstab(df['style'], df['mood'])\n",
        "sns.heatmap(style_mood_crosstab, annot=True, fmt='d', cmap='Oranges', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Style-Mood Distribution')\n",
        "axes[1, 0].set_xlabel('Mood')\n",
        "axes[1, 0].set_ylabel('Style')\n",
        "\n",
        "# 4. Quality by cultural context\n",
        "cultural_quality = df.groupby(['style', 'mood'])['quality_score'].mean().unstack()\n",
        "sns.heatmap(cultural_quality, annot=True, fmt='.3f', cmap='viridis', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Average Quality by Style-Mood')\n",
        "axes[1, 1].set_xlabel('Mood')\n",
        "axes[1, 1].set_ylabel('Style')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Regional cultural diversity\n",
        "print(\"\\n=== REGIONAL CULTURAL DIVERSITY ===\")\n",
        "regional_diversity = df.groupby('region').agg({\n",
        "    'raga': lambda x: x.nunique(),\n",
        "    'mood': lambda x: x.nunique(),\n",
        "    'time_of_day': lambda x: x.nunique(),\n",
        "    'filename': 'count'\n",
        "})\n",
        "regional_diversity.columns = ['Unique_Ragas', 'Unique_Moods', 'Unique_Times', 'Total_Images']\n",
        "regional_diversity['Diversity_Index'] = (regional_diversity['Unique_Ragas'] * \n",
        "                                        regional_diversity['Unique_Moods'] * \n",
        "                                        regional_diversity['Unique_Times']) / regional_diversity['Total_Images']\n",
        "print(regional_diversity)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## 7. Text Prompt Analysis {#prompt-analysis}"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Generate sample prompts for analysis\n",
        "def generate_sample_prompts(df):\n",
        "    \"\"\"Generate sample prompts based on metadata.\"\"\"\n",
        "    prompts = []\n",
        "    \n",
        "    prompt_templates = [\n",
        "        \"A {style} style ragamala painting depicting raga {raga}\",\n",
        "        \"An exquisite {style} miniature from {period} illustrating {raga} raga\",\n",
        "        \"Traditional {style} artwork showing raga {raga} in {mood} mood\",\n",
        "        \"{period} {style} painting of raga {raga} during {time_of_day}\",\n",
        "        \"Classical Indian {style} ragamala depicting the {mood} essence of {raga}\"\n",
        "    ]\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        template = np.random.choice(prompt_templates)\n",
        "        prompt = template.format(\n",
        "            style=row['style'],\n",
        "            raga=row['raga'],\n",
        "            period=row['period'].replace('_', ' '),\n",
        "            mood=row['mood'],\n",
        "            time_of_day=row['time_of_day']\n",
        "        )\n",
        "        prompts.append(prompt)\n",
        "    \n",
        "    return prompts\n",
        "\n",
        "# Generate prompts\n",
        "df['prompt'] = generate_sample_prompts(df)\n",
        "\n",
        "print(\"=== TEXT PROMPT ANALYSIS ===\")\n",
        "print(f\"Generated {len(df['prompt'])} prompts\")\n",
        "print(\"\\nSample prompts:\")\n",
        "for i in range(5):\n",
        "    print(f\"{i+1}. {df['prompt'].iloc[i]}\")\n",
        "\n",
        "# Analyze prompt characteristics\n",
        "df['prompt_length'] = df['prompt'].str.len()\n",
        "df['prompt_word_count'] = df['prompt'].str.split().str.len()\n",
        "\n",
        "print(f\"\\nPrompt Statistics:\")\n",
        "print(f\"Average length: {df['prompt_length'].mean():.1f} characters\")\n",
        "print(f\"Average word count: {df['prompt_word_count'].mean():.1f} words\")\n",
        "print(f\"Length range: {df['prompt_length'].min()} - {df['prompt_length'].max()} characters\")\n",
        "\n",
        "# Word frequency analysis\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    \n",
        "    # Combine all prompts\n",
        "    all_text = ' '.join(df['prompt'])\n",
        "    \n",
        "    # Tokenize and clean\n",
        "    tokens = word_tokenize(all_text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    \n",
        "    # Count word frequencies\n",
        "    word_freq = Counter(filtered_tokens)\n",
        "    \n",
        "    print(f\"\\nMost common words in prompts:\")\n",
        "    for word, count in word_freq.most_common(15):\n",
        "        print(f\"{word}: {count}\")\n",
        "    \n",
        "    # Create word cloud\n",
        "    if len(word_freq) > 0:\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "        \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title('Word Cloud of Prompt Terms', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"NLTK not available for text analysis\")\n",
        "\n",
        "# Prompt length analysis by style and raga\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Prompt length by style\n",
        "df.boxplot(column='prompt_length', by='style', ax=axes[0])\n",
        "axes[0].set_title('Prompt Length by Style')\n",
        "axes[0].set_xlabel('Style')\n",
        "axes[0].set_ylabel('Prompt Length (characters)')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Word count distribution\n",
        "axes[1].hist(df['prompt_word_count'], bins=20, alpha=0.7, color='green', edgecolor='black')\n",
        "axes[1].axvline(df['prompt_word_count'].mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {df[\"prompt_word_count\"].mean():.1f}')\n",
        "axes[1].set_xlabel('Word Count')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Prompt Word Count Distribution')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## 8. Data Quality Assessment {#quality-assessment}"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Comprehensive data quality assessment\n",
        "print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
        "\n",
        "# 1. Missing data analysis\n",
        "missing_analysis = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
        "    'Data_Type': df.dtypes\n",
        "})\n",
        "missing_analysis = missing_analysis[missing_analysis['Missing_Count'] > 0]\n",
        "\n",
        "if not missing_analysis.empty:\n",
        "    print(\"Missing Data Analysis:\")\n",
        "    print(missing_analysis)\n",
        "else:\n",
        "    print(\"No missing data found!\")\n",
        "\n",
        "# 2. Duplicate analysis\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"\\nDuplicate rows: {duplicate_count}\")\n",
        "\n",
        "# Check for duplicate filenames\n",
        "duplicate_filenames = df['filename'].duplicated().sum()\n",
        "print(f\"Duplicate filenames: {duplicate_filenames}\")\n",
        "\n",
        "# 3. Outlier detection\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, len(outliers)\n",
        "\n",
        "print(\"\\n=== OUTLIER ANALYSIS ===\")\n",
        "numeric_columns = ['width', 'height', 'file_size_mb', 'quality_score', 'aspect_ratio']\n",
        "\n",
        "outlier_summary = {}\n",
        "for col in numeric_columns:\n",
        "    outliers, count = detect_outliers_iqr(df, col)\n",
        "    outlier_summary[col] = count\n",
        "    print(f\"{col}: {count} outliers ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "# 4. Data consistency checks\n",
        "print(\"\\n=== DATA CONSISTENCY CHECKS ===\")\n",
        "\n",
        "# Check aspect ratio consistency\n",
        "calculated_aspect_ratio = df['width'] / df['height']\n",
        "aspect_ratio_diff = abs(df['aspect_ratio'] - calculated_aspect_ratio)\n",
        "inconsistent_aspect_ratio = (aspect_ratio_diff > 0.01).sum()\n",
        "print(f\"Inconsistent aspect ratios: {inconsistent_aspect_ratio}\")\n",
        "\n",
        "# Check total pixels consistency\n",
        "calculated_total_pixels = df['width'] * df['height']\n",
        "pixels_diff = abs(df['total_pixels'] - calculated_total_pixels)\n",
        "inconsistent_pixels = (pixels_diff > 1000).sum()\n",
        "print(f\"Inconsistent total pixels: {inconsistent_pixels}\")\n",
        "\n",
        "# 5. Quality score distribution analysis\n",
        "print(\"\\n=== QUALITY SCORE ANALYSIS ===\")\n",
        "quality_stats = df['quality_score'].describe()\n",
        "print(quality_stats)\n",
        "\n",
        "# Quality score by various factors\n",
        "quality_by_style = df.groupby('style')['quality_score'].agg(['mean', 'std', 'count'])\n",
        "print(\"\\nQuality by Style:\")\n",
        "print(quality_by_style)\n",
        "\n",
        "# Create quality assessment visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Data Quality Assessment', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Outlier visualization\n",
        "outlier_counts = list(outlier_summary.values())\n",
        "outlier_columns = list(outlier_summary.keys())\n",
        "axes[0, 0].bar(outlier_columns, outlier_counts, color='red', alpha=0.7)\n",
        "axes[0, 0].set_title('Outlier Count by Column')\n",
        "axes[0, 0].set_ylabel('Number of Outliers')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 2. Quality score distribution\n",
        "axes[0, 1].hist(df['quality_score'], bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
        "axes[0, 1].axvline(df['quality_score'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {df[\"quality_score\"].mean():.3f}')\n",
        "axes[0, 1].axvline(df['quality_score'].median(), color='orange', linestyle='--', \n",
        "                   label=f'Median: {df[\"quality_score\"].median():.3f}')\n",
        "axes[0, 1].set_xlabel('Quality Score')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Quality Score Distribution')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. File size vs quality scatter\n",
        "axes[1, 0].scatter(df['file_size_mb'], df['quality_score'], alpha=0.6)\n",
        "axes[1, 0].set_xlabel('File Size (MB)')\n",
        "axes[1, 0].set_ylabel('Quality Score')\n",
        "axes[1, 0].set_title('File Size vs Quality Score')\n",
        "\n",
        "# Add correlation coefficient\n",
        "corr_coef = df['file_size_mb'].corr(df['quality_score'])\n",
        "axes[1, 0].text(0.05, 0.95, f'Correlation: {corr_coef:.3f}', \n",
        "                transform=axes[1, 0].transAxes, bbox=dict(boxstyle='round', facecolor='white'))\n",
        "\n",
        "# 4. Resolution vs quality\n",
        "df.boxplot(column='quality_score', by='resolution_category', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Quality Score by Resolution Category')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Data completeness score\n",
        "completeness_score = (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
        "print(f\"\\nOverall data completeness: {completeness_score:.2f}%\")\n",
        "\n",
        "# 7. Recommendations for data cleaning\n",
        "print(\"\\n=== DATA CLEANING RECOMMENDATIONS ===\")\n",
        "recommendations = []\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    recommendations.append(f\"Remove {duplicate_count} duplicate rows\")\n",
        "\n",
        "if duplicate_filenames > 0:\n",
        "    recommendations.append(f\"Investigate {duplicate_filenames} duplicate filenames\")\n",
        "\n",
        "for col, count in outlier_summary.items():\n",
        "    if count > len(df) * 0.05:  # More than 5% outliers\n",
        "        recommendations.append(f\"Review {count} outliers in {col} column\")\n",
        "\n",
        "if df['quality_score'].min() < 0.5:\n",
        "    low_quality_count = (df['quality_score'] < 0.5).sum()\n",
        "    recommendations.append(f\"Consider removing {low_quality_count} low-quality images (score < 0.5)\")\n",
        "\n",
        "if recommendations:\n",
        "    for i, rec in enumerate(recommendations, 1):\n",
        "        print(f\"{i}. {rec}\")\n",
        "else:\n",
        "    print(\"No major data quality issues detected!\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## 9. Insights and Recommendations {#insights}"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
        "# Generate comprehensive insights and recommendations\n",
        "print(\"=== COMPREHENSIVE INSIGHTS AND RECOMMENDATIONS ===\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATASET INSIGHTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Dataset size and distribution insights\n",
        "print(f\"\\n1. DATASET OVERVIEW:\")\n",
        "print(f\"   - Total images: {len(df):,}\")\n",
        "print(f\"   - Unique ragas: {df['raga'].nunique()}\")\n",
        "print(f\"   - Unique styles: {df['style'].nunique()}\")\n",
        "print(f\"   - Time periods covered: {df['period'].nunique()}\")\n",
        "print(f\"   - Average quality score: {df['quality_score'].mean():.3f}\")\n",
        "\n",
        "# 2. Balance analysis\n",
        "print(f\"\\n2. DATA BALANCE ANALYSIS:\")\n",
        "raga_balance = df['raga'].value_counts()\n",
        "style_balance = df['style'].value_counts()\n",
        "\n",
        "raga_imbalance_ratio = raga_balance.max() / raga_balance.min()\n",
        "style_imbalance_ratio = style_balance.max() / style_balance.min()\n",
        "\n",
        "print(f\"   - Raga imbalance ratio: {raga_imbalance_ratio:.2f}:1\")\n",
        "print(f\"   - Style imbalance ratio: {style_imbalance_ratio:.2f}:1\")\n",
        "print(f\"   - Most common raga: {raga_balance.index[0]} ({raga_balance.iloc[0]} images)\")\n",
        "print(f\"   - Least common raga: {raga_balance.index[-1]} ({raga_balance.iloc[-1]} images)\")\n",
        "\n",
        "# 3. Technical quality insights\n",
        "print(f\"\\n3. TECHNICAL QUALITY:\")\n",
        "high_quality_count = (df['quality_score'] >= 0.8).sum()\n",
        "low_quality_count = (df['quality_score'] < 0.6).sum()\n",
        "\n",
        "print(f\"   - High quality images (≥0.8): {high_quality_count} ({high_quality_count/len(df)*100:.1f}%)\")\n",
        "print(f\"   - Low quality images (<0.6): {low_quality_count} ({low_quality_count/len(df)*100:.1f}%)\")\n",
        "print(f\"   - Average resolution: {df['total_pixels'].mean()/1000000:.1f} megapixels\")\n",
        "print(f\"   - Average file size: {df['file_size_mb'].mean():.2f} MB\")\n",
        "\n",
        "# 4. Cultural diversity insights\n",
        "print(f\"\\n4. CULTURAL DIVERSITY:\")\n",
        "mood_diversity = df['mood'].nunique()\n",
        "time_diversity = df['time_of_day'].nunique()\n",
        "\n",
        "print(f\"   - Unique moods represented: {mood_diversity}\")\n",
        "print(f\"   - Time periods represented: {time_diversity}\")\n",
        "print(f\"   - Most common mood: {df['mood'].value_counts().index[0]}\")\n",
        "print(f\"   - Most common time: {df['time_of_day'].value_counts().index[0]}\")\n",
        "\n",
        "# Generate recommendations\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RECOMMENDATIONS FOR SDXL FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Data preprocessing recommendations\n",
        "print(\"\\n1. DATA PREPROCESSING:\")\n",
        "if raga_imbalance_ratio > 3:\n",
        "    print(\"   - Apply data augmentation to balance raga distribution\")\n",
        "    print(\"   - Consider weighted sampling during training\")\n",
        "\n",
        "if style_imbalance_ratio > 2:\n",
        "    print(\"   - Implement style-aware batch sampling\")\n",
        "    print(\"   - Use stratified splitting for train/val/test\")\n",
        "\n",
        "if low_quality_count > len(df) * 0.1:\n",
        "    print(f\"   - Filter out {low_quality_count} low-quality images\")\n",
        "    print(\"   - Apply quality-based weighting in loss function\")\n",
        "\n",
        "print(\"   - Standardize image resolution to 1024x1024 for SDXL\")\n",
        "print(\"   - Apply consistent color space normalization\")\n",
        "\n",
        "# Training strategy recommendations\n",
        "print(\"\\n2. TRAINING STRATEGY:\")\n",
        "print(\"   - Use LoRA fine-tuning with rank 64-128 for efficiency\")\n",
        "print(\"   - Implement cultural conditioning in text encoder\")\n",
        "print(\"   - Apply gradient accumulation for effective batch size of 16-32\")\n",
        "print(\"   - Use mixed precision (fp16) to reduce memory usage\")\n",
        "\n",
        "if raga_imbalance_ratio > 2:\n",
        "    print(\"   - Implement class-balanced sampling\")\n",
        "    print(\"   - Use focal loss to handle class imbalance\")\n",
        "\n",
        "# Model architecture recommendations\n",
        "print(\"\\n3. MODEL ARCHITECTURE:\")\n",
        "print(\"   - Fine-tune SDXL 1.0 base model with LoRA\")\n",
        "print(\"   - Add cultural embedding layers for raga/style conditioning\")\n",
        "print(\"   - Implement attention mechanisms for cultural features\")\n",
        "print(\"   - Use classifier-free guidance for better prompt adherence\")\n",
        "\n",
        "# Data augmentation recommendations\n",
        "print(\"\\n4. DATA AUGMENTATION:\")\n",
        "print(\"   - Apply rotation (±10°) while preserving cultural elements\")\n",
        "print(\"   - Use color jittering to increase style variation\")\n",
        "print(\"   - Implement crop-and-resize for composition diversity\")\n",
        "print(\"   - Avoid flipping to preserve text and cultural symbols\")\n",
        "\n",
        "# Evaluation recommendations\n",
        "print(\"\\n5. EVALUATION STRATEGY:\")\n",
        "print(\"   - Use FID score for overall image quality\")\n",
        "print(\"   - Implement CLIP score for text-image alignment\")\n",
        "print(\"   - Create cultural authenticity metrics\")\n",
        "print(\"   - Conduct human evaluation with art experts\")\n",
        "\n",
        "# Deployment recommendations\n",
        "print(\"\\n6. DEPLOYMENT ON EC2:\")\n",
        "print(\"   - Use g5.2xlarge or g4dn.xlarge instances\")\n",
        "print(\"   - Implement model serving with FastAPI\")\n",
        "print(\"   - Set up auto-scaling based on demand\")\n",
        "print(\"   - Use S3 for model storage and generated images\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CULTURAL CONDITIONING INSIGHTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Analyze cultural patterns for conditioning\n",
        "style_raga_patterns = df.groupby(['style', 'raga']).size().unstack(fill_value=0)\n",
        "print(\"\\nStyle-Raga Co-occurrence Matrix:\")\n",
        "print(style_raga_patterns)\n",
        "\n",
        "# Calculate cultural affinity scores\n",
        "cultural_affinity = {}\n",
        "for style in df['style'].unique():\n",
        "    style_data = df[df['style'] == style]\n",
        "    raga_dist = style_data['raga'].value_counts(normalize=True)\n",
        "    cultural_affinity[style] = raga_dist.to_dict()\n",
        "\n",
        "print(\"\\nCultural Affinity Scores (Raga distribution by Style):\")\n",
        "for style, affinities in cultural_affinity.items():\n",
        "    print(f\"\\n{style.upper()}:\")\n",
        "    for raga, score in sorted(affinities.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
        "        print(f\"  {raga}: {score:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RECOMMENDATIONS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "final_recommendations = [\n",
        "    \"1. Dataset: Balance raga distribution through augmentation and weighted sampling\",\n",
        "    \"2. Preprocessing: Standardize to 1024x1024, apply quality filtering (>0.6)\",\n",
        "    \"3. Model: Use SDXL 1.0 + LoRA (rank 64) with cultural conditioning layers\",\n",
        "    \"4. Training: Batch size 4, gradient accumulation 4, mixed precision fp16\",\n",
        "    \"5. Prompting: Implement cultural templates with raga/style/period context\",\n",
        "    \"6. Evaluation: Multi-metric approach including cultural authenticity scoring\",\n",
        "    \"7. Deployment: EC2 g5.2xlarge for training, g4dn.xlarge for inference\",\n",
        "    \"8. Monitoring: Use W&B for experiment tracking and model versioning\"\n",
        "]\n",
        "\n",
        "for rec in final_recommendations:\n",
        "    print(f\"   {rec}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ANALYSIS COMPLETE - READY FOR SDXL FINE-TUNING\")\n",
        "print(f\"{'='*60}\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
        "## Summary and Next Steps\n",
        "\n",
        "This comprehensive EDA has revealed key insights about our Ragamala painting dataset:\n",
        "\n",
        "### Key Findings:\n",
        "1. Dataset Composition: Well-distributed across major styles (Rajput, Pahari, Deccan, Mughal)\n",
        "2. Cultural Diversity: Good representation of different ragas and time periods\n",
        "3. Technical Quality: Majority of images meet quality standards for training\n",
        "4. Cultural Patterns: Strong associations between certain ragas and styles\n",
        "\n",
        "### Recommended Next Steps:\n",
        "1. Data Preprocessing: Implement the cleaning recommendations\n",
        "2. Prompt Engineering: Develop cultural conditioning templates\n",
        "3. Model Architecture: Design SDXL + LoRA with cultural embeddings\n",
        "4. Training Pipeline: Set up distributed training on EC2\n",
        "5. Evaluation Framework: Implement cultural authenticity metrics\n",
        "\n",
        "### EC2 Deployment Strategy:\n",
        "- Training: g5.2xlarge with 500GB EBS storage\n",
        "- Inference: g4dn.xlarge with auto-scaling\n",
        "- Storage: S3 for datasets and model artifacts\n",
        "- Monitoring: CloudWatch + W&B integration\n",
        "\n",
        "This analysis provides a solid foundation for building a culturally-aware SDXL model that can generate authentic Ragamala paintings while respecting traditional artistic conventions."
     ]
    }
 ],
 "metadata": {
    "kernelspec": {
     "display_name": "Python 3",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
        "name": "ipython",
        "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.10.0"
    }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
